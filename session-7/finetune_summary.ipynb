{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024S2/blob/main/session-7/finetune_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28304084-e6f6-45cf-9076-8dc439ef13e4",
      "metadata": {
        "id": "28304084-e6f6-45cf-9076-8dc439ef13e4"
      },
      "source": [
        "# Fine-Tune a Causal Language Model for Dialogue Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc73e16b-474b-40e9-bbaf-6f43479f018b",
      "metadata": {
        "id": "dc73e16b-474b-40e9-bbaf-6f43479f018b"
      },
      "source": [
        "In this exercise, you will fine-tune Meta's Llama 3.2 LLM for enhanced dialogue summarization. We will explore how to use the Huggingface TRL (Transformer Reinforcement Learning) library to help us to perform Supervised Finetuning (SFT).  We will explore the use of Parameter Efficient Fine-Tuning (PEFT) for efficient and fast finetuning, and evaluate the resulting model using ROUGE metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f022ce84-192b-4f62-8385-2cf571289117",
      "metadata": {
        "id": "f022ce84-192b-4f62-8385-2cf571289117"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q accelerate peft bitsandbytes transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84ed7ea-3707-445a-a266-e4181546f2a7",
      "metadata": {
        "id": "c84ed7ea-3707-445a-a266-e4181546f2a7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12011343-2a10-426a-bf38-6fc3f43766f7",
      "metadata": {
        "id": "12011343-2a10-426a-bf38-6fc3f43766f7"
      },
      "source": [
        "## Templating Instruction Data\n",
        "\n",
        "To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template.\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/chat_template.png?raw=true\" />\n",
        "\n",
        "This chat template differentiates between what the LLM has generated and what the user has generated. May LLM chat models that are available on HuggingFace comes with built-in chat template that you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16862cf7-29c7-47b0-8f33-053428b2bd78",
      "metadata": {
        "id": "16862cf7-29c7-47b0-8f33-053428b2bd78"
      },
      "outputs": [],
      "source": [
        "# This is the chat model of TinyLlama. We only load it because we want to use it's chat template to format our data\n",
        "chat_model=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "template_tokenizer = AutoTokenizer.from_pretrained(chat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fff006d-9431-4748-9f4f-b0ce081d7295",
      "metadata": {
        "id": "3fff006d-9431-4748-9f4f-b0ce081d7295"
      },
      "outputs": [],
      "source": [
        "template_tokenizer.get_chat_template()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e60b383b-aad1-4ea6-a576-62d27aa44167",
      "metadata": {
        "id": "e60b383b-aad1-4ea6-a576-62d27aa44167"
      },
      "source": [
        "You can see that the template expects the prompt to include fields like role, content, and with content demarcated by `|user|`, `|assistant|` and `|system|`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c592d791-896d-41d2-8db8-b1d73e49971e",
      "metadata": {
        "id": "c592d791-896d-41d2-8db8-b1d73e49971e"
      },
      "source": [
        "### Format the data according to chat template\n",
        "\n",
        "Let's download our data and format them according to the template given. We select a subset of 6000 samples to reduce training time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5511e86-af57-4f67-8546-75783fb92253",
      "metadata": {
        "id": "d5511e86-af57-4f67-8546-75783fb92253"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset_train = load_dataset(dataset_name, split='train[:3000]')\n",
        "dataset_val = load_dataset(dataset_name, split='validation')\n",
        "dataset_test = load_dataset(dataset_name, split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa20ede9-b3b7-411e-ae5b-fdc82bcf9a29",
      "metadata": {
        "id": "fa20ede9-b3b7-411e-ae5b-fdc82bcf9a29"
      },
      "outputs": [],
      "source": [
        "dataset_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a6ebda-b3a7-4b57-a43d-7ec54552a04c",
      "metadata": {
        "id": "70a6ebda-b3a7-4b57-a43d-7ec54552a04c"
      },
      "source": [
        "Note that the completed prompt is put under 'text' field of the json. This is the default field that model will look for the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6928d0-c83b-41e8-a2c3-a63827b6c851",
      "metadata": {
        "id": "7c6928d0-c83b-41e8-a2c3-a63827b6c851"
      },
      "outputs": [],
      "source": [
        "def format_chat_template(row):\n",
        "    user_prompt = (\n",
        "        f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
        "    )\n",
        "    user_prompt = user_prompt.format(dialog = row[\"dialogue\"])\n",
        "    row_json = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant\" },\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "               {\"role\": \"assistant\", \"content\": row[\"summary\"]}]\n",
        "\n",
        "    prompt = template_tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "    # print(prompt)\n",
        "    return {\"text\": prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debee8c3-bec5-420a-9535-e6bbb7afe3f8",
      "metadata": {
        "id": "debee8c3-bec5-420a-9535-e6bbb7afe3f8"
      },
      "outputs": [],
      "source": [
        "dataset_train = dataset_train.map(format_chat_template, remove_columns=list(dataset_train.features))\n",
        "dataset_val = dataset_val.map(format_chat_template, remove_columns=list(dataset_val.features))\n",
        "dataset_test = dataset_test.map(format_chat_template, remove_columns=list(dataset_test.features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2265ec5-a1d7-429a-b4d5-4757377b2ced",
      "metadata": {
        "id": "a2265ec5-a1d7-429a-b4d5-4757377b2ced"
      },
      "source": [
        "Using the \"text\" column, we can explore these formatted prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2558fe-dd49-436e-82ea-220564ea69d0",
      "metadata": {
        "id": "ed2558fe-dd49-436e-82ea-220564ea69d0"
      },
      "outputs": [],
      "source": [
        "dataset_train[0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33dc659d-5e7c-4114-a4ae-2c27ccf4a2c1",
      "metadata": {
        "id": "33dc659d-5e7c-4114-a4ae-2c27ccf4a2c1"
      },
      "source": [
        "### Model Quantization\n",
        "\n",
        "Now that we have our data, we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. We use the\n",
        "bitsandbytes package to compress the pretrained model to a 4-bit representation.\n",
        "\n",
        "In BitsAndBytesConfig, you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac96ae31-af8a-40f7-a39e-b4649ba1b837",
      "metadata": {
        "id": "ac96ae31-af8a-40f7-a39e-b4649ba1b837"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# 4-bit quantization configuration - Q in QLoRA\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # Use 4-bit precision model loading\n",
        "    bnb_4bit_quant_type=\"nf4\", # Quantization type\n",
        "    bnb_4bit_compute_dtype=\"float16\", # Compute dtype\n",
        "    bnb_4bit_use_double_quant=True, # Apply nested quantization\n",
        ")\n",
        "\n",
        "# Load the model to train on the GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda:0\",\n",
        "    # Leave this out for regular SFT\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e19a36-b88b-4c9b-94f1-fc7c4775bac5",
      "metadata": {
        "id": "37e19a36-b88b-4c9b-94f1-fc7c4775bac5"
      },
      "source": [
        "### Test the Model with Zero Shot Inferencing\n",
        "\n",
        "Let's test the base model (non-instruction tuned model) with zero shot inferencing (i.e. ask it to summarize without giving any example. You can see that the model struggles to summarize the dialogue compared to the baseline summary, and it is just repeating the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f9e56be-0cfc-46bd-bbb6-0b3a6e7cc821",
      "metadata": {
        "id": "0f9e56be-0cfc-46bd-bbb6-0b3a6e7cc821"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "#Person1#: I have a problem with my cable.\n",
        "#Person2#: What about it?\n",
        "#Person1#: My cable has been out for the past week or so.\n",
        "#Person2#: The cable is down right now. I am very sorry.\n",
        "#Person1#: When will it be working again?\n",
        "#Person2#: It should be back on in the next couple of days.\n",
        "#Person1#: Do I still have to pay for the cable?\n",
        "#Person2#: We're going to give you a credit while the cable is down.\n",
        "#Person1#: So, I don't have to pay for it?\n",
        "#Person2#: No, not until your cable comes back on.\n",
        "#Person1#: Okay, thanks for everything.\n",
        "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():   # no gradient update\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9cad99-f525-4de6-9458-23affaa28b47",
      "metadata": {
        "id": "1c9cad99-f525-4de6-9458-23affaa28b47"
      },
      "source": [
        "### LoRA Configuration\n",
        "\n",
        "We will be using LoRA to train our model. LoRA is supported in Hugging Face's PEFT library.\n",
        "Here are some explanation about the parameters used in the LoRA:\n",
        "- `r` - This is the rank of the compressed matrices. Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n",
        "- `lora_alpha` - Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n",
        "- `target_modules` - Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b180947c-ddc3-4bda-980f-5afd27199d21",
      "metadata": {
        "id": "b180947c-ddc3-4bda-980f-5afd27199d21"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "# Prepare LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,  # LoRA Scaling\n",
        "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
        "    r=64,  # Rank\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=  # Layers to target\n",
        "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
        ")\n",
        "\n",
        "# prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f12c6f-df14-4b1d-8e1d-a15c22bd029e",
      "metadata": {
        "id": "d6f12c6f-df14-4b1d-8e1d-a15c22bd029e"
      },
      "source": [
        "### Training Configuration\n",
        "\n",
        "Next we need to set our training configuration. Since we are going to use SFTTrainer, we can specify the training arguments in SFTConfig.\n",
        "\n",
        "Note that we set `fp16` to True for mixed-precision training. If you are using Ampere and newer GPU architecture, you can set bf16 to better accuracy and faster training.\n",
        "\n",
        "Also we specify `packing=True`. Instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with a EOS token in between and cut chunks of the context size to fill the batch without any padding.\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/packing.png?raw=1\" width=\"700\"/>\n",
        "\n",
        "Internally, a [`ConstantLengthDataset`](https://huggingface.co/docs/trl/en/sft_trainer#trl.trainer.ConstantLengthDataset) is being created so we can iterate over the dataset on fixed-length sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5f9d4e-3472-4898-9be2-f9f40a27fe32",
      "metadata": {
        "id": "de5f9d4e-3472-4898-9be2-f9f40a27fe32"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Configure the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# where to write the checkpoint to\n",
        "output_dir = \"./results\"\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # num_train_epochs=1,\n",
        "    logging_steps=5,\n",
        "    max_steps=30,\n",
        "    bf16=True,\n",
        "    # fp16=True\n",
        "    gradient_checkpointing=True,\n",
        "    resume_from_checkpoint=True,\n",
        "    packing=True,\n",
        "    eval_packing=False,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps=5,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=5,\n",
        "    run_name=\"llama3.2-summarize\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_q15CbK4slTM"
      },
      "id": "_q15CbK4slTM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9290aac1-0c8e-4d90-ba70-1b03757c9d32",
      "metadata": {
        "id": "9290aac1-0c8e-4d90-ba70-1b03757c9d32"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_val,\n",
        "    # dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    # Leave this out for regular SFT\n",
        "    peft_config=peft_config,\n",
        "    args=sft_config\n",
        " )\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cac77f-d8b5-4c66-bfcf-5c95c0037ae6",
      "metadata": {
        "id": "36cac77f-d8b5-4c66-bfcf-5c95c0037ae6"
      },
      "outputs": [],
      "source": [
        "# Save QLoRA weights\n",
        "trainer.model.save_pretrained(\"Llama-3.2-1B-Summarizer-QLoRA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fcace21-7c4a-4b4a-9e5c-7383de2dd1a9",
      "metadata": {
        "id": "5fcace21-7c4a-4b4a-9e5c-7383de2dd1a9"
      },
      "outputs": [],
      "source": [
        "def get_max_context_length(model):\n",
        "\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max context lenth: {max_length} in {length_setting}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max context length: {max_length}\")\n",
        "\n",
        "    return max_length\n",
        "\n",
        "max_context_length = get_max_context_length(model)\n",
        "print('Maximum Context length: ', max_context_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b0f3bc-21fe-43b4-9dcb-f7fac196c4ba",
      "metadata": {
        "id": "c2b0f3bc-21fe-43b4-9dcb-f7fac196c4ba"
      },
      "source": [
        "### Merge Weights\n",
        "\n",
        "After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe985335-acfa-4ea2-95de-05fd4088286f",
      "metadata": {
        "id": "fe985335-acfa-4ea2-95de-05fd4088286f"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"Llama-3.2-1B-Summarizer-QLoRA\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Merge LoRA and base model\n",
        "merged_model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a981eebf-12cc-45b6-a05b-f54e2a59eb2e",
      "metadata": {
        "id": "a981eebf-12cc-45b6-a05b-f54e2a59eb2e"
      },
      "source": [
        "After merging the adapter with the base model, we can use it with the prompt template that we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db04c34-623f-4bec-8a85-b5e3024862ba",
      "metadata": {
        "scrolled": true,
        "id": "7db04c34-623f-4bec-8a85-b5e3024862ba"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"<|user|>\n",
        "Summarize this dialog:\n",
        "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "B: I’m pretty sure I am. What’s up?\n",
        "A: Can you go with me to the animal shelter?.\n",
        "B: What do you want to do?\n",
        "A: I want to get a puppy for my son.\n",
        "B: That will make him so happy.\n",
        "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "A: I'll get him one of those little dogs.\n",
        "B: One that won't grow up too big;-)\n",
        "A: And eat too much;-))\n",
        "B: Do you know which one he would like?\n",
        "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "B: I bet you had to drag him away.\n",
        "A: He wanted to take it home right away ;-).\n",
        "B: I wonder what he'll name it.\n",
        "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "---\n",
        "Summary:</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "from transformers import TextStreamer\n",
        "from transformers import pipeline\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Run our instruction-tuned model\n",
        "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "print(pipe(eval_prompt)[0][\"generated_text\"])\n",
        "\n",
        "\n",
        "# #Streaming support\n",
        "# streamer = TextStreamer(tokenizer)\n",
        "# merged_model.eval()\n",
        "# with torch.no_grad():\n",
        "#     merged_model.generate(**model_input, streamer=streamer, max_length=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e586894d-a59a-41fc-af0b-1baa67e734b0",
      "metadata": {
        "id": "e586894d-a59a-41fc-af0b-1baa67e734b0"
      },
      "source": [
        "Good reference:\n",
        "\n",
        "https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-tune-an-LLM-Part-3-The-HuggingFace-Trainer--Vmlldzo1OTEyNjMy\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}